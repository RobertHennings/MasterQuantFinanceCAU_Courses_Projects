---
title: "Solution Tutorial 4"
output:
  pdf_document: default
  html_notebook: default
---

# 1. Inverse transformation method

If the univariate continuous random variable X has a cumulative distribution function $F_X$, then $Y=F_X(X)$ is a standard uniform random variable. Conversely, if Y has a uniform distribution on [0, 1] and if X has an invertible cumulative distribution $F_{X}$, then the random variable $F_{X}^{{-1}}(Y)$ has the same distribution as X. Note that $Q:=F_X^{-1}$ is the quantile function.


Assume X follows an exponential distribution. Draw a sample Y from the standard uniform distribution and use the theorem above to create an exponential distributed sample X based on Y. Visualize your results for different rates ($\lambda\in\{0.01,0.1,1,10\}$) in one graph with four subplots. To this end use 1000 Monte-Carlo-replications.

```{r}
# define parameters
NMC = 1000
l = c(0.01, 0.1, 1, 10)

# draw samples
set.seed(666)
sample = matrix(0, nrow = NMC, ncol = length(l))
for (i in 1:length(l)){
  sample[,i] = qexp(runif(NMC), rate = l[i])
}

# plot -> histograms
par(mfrow = c(2,2))
for (i in 1:length(l)){
  # empirical
  #hist(sample[,i], breaks = "Scott", xlab = "x", ylab = "density", probability = T, main = substitute(paste(lambda, '= ', u, sep=''), list(u = l[i])))
    hist(sample[,i], breaks = "Scott", xlab = "x", ylab = "density", probability = T, main=paste("Lambda =", l[i]))

  # theoretical
  x = seq(from = 0, to = max(sample[,i]), by = 0.01)
  lines(x, dexp(x, rate = l[i]), col = "red", lwd = 2)
}
title("Simulation of Integral Transform", line=-1, outer = T)




# plot -> qq-plot
for (i in 1:length(l)){
  # empirical
  qqplot(qexp(ppoints(NMC), rate = l[i]),sample[,i], xlab = "theoretical quantiles", ylab = "empirical quantiles", main = substitute(paste(lambda, '= ', u, sep=''), list(u = l[i])))
  # theoretical
  x = seq(from = 0, to = max(sample[,i]), by = 0.01)
  lines(x, x, col = "red", lwd = 2)
}
title("Simulation of Integral Transform", line = -27, outer = TRUE)
par(mfrow = c(1,1))
```


# 2. Existence of moments

It was shown in the lecture that the Cauchy distribution (a student-t distribution with one degree of freedom) does not have any moment.

### a)
Draw a large ($n \approx 1000$) sample from a Cauchy distribution. Verify that this sample is indeed Cauchy distributed with the help of a histogram.

```{r}
# draw sample
n = 100000
#set.seed(123)
sample = rt(n, df = 1)

# plot
hist(sample, breaks = "Scott", xlab = "", ylab = "", probability = T, main = "Simulation of Cauchy distribution")
x = seq(min(sample), max(sample), by = 0.1)
lines(x, dt(x, df = 1), col = "blue", lwd = 2)
```

### b) 
Compute the partial means for this sample. The partial mean of a sample $X_1, ... , X_n$ is defined by $\overline{x}_j = \frac{1}{j}\sum_{i=1}^j x_i$.

```{r}
# compute partial means 
partmeans = cumsum(sample)/(1:length(sample))
```

### c)
Draw the partial means as a function of n. 

```{r}
# plot
plot(1:n, partmeans, type = "l", lwd = 2, main = "Partial means of Cauchy", xlab = "n")
```
